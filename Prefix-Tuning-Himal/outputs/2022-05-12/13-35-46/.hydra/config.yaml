task_name: boolq
method: prompt_tuning
plm: t5-base
logging_steps: 50
eval_steps: 300
eval_batch_size: 512
boolq:
  prefix_tuning:
    gpt2:
      tokenizer_name: gpt2
      learning_rate: 0.01
      num_train_epochs: 1000
      max_seq_length: 128
      n_prompt_tokens: 20
      init_from_vocab: true
      random_range: 0.5
    t5-base:
      tokenizer_name: t5-base
      learning_rate: 0.01
      num_train_epochs: 10
      max_seq_length: 128
      n_prompt_tokens: 100
      init_from_vocab: false
      random_range: 0.5
      num_warmup_steps: 0
      num_training_steps: 500
    roberta-base:
      tokenizer_name: roberta-base
      learning_rate: 0.001
      num_train_epochs: 1000
      max_seq_length: 128
      n_prompt_tokens: 20
      init_from_vocab: true
      random_range: 0.5
    t5-large:
      tokenizer_name: t5-base
      learning_rate: 0.001
      num_train_epochs: 1000
      max_seq_length: 128
      n_prompt_tokens: 20
      init_from_vocab: true
      random_range: 0.5
